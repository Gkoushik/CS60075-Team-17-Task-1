{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FinalNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uFa0yISJ8Pk"
      },
      "source": [
        "!pip -q install transformers"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzRUkrgjnFyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6bb142-09c6-47e4-f5bd-66369e060a76"
      },
      "source": [
        "# Gets the data\n",
        "\n",
        "%%bash\n",
        "rm -r sample_data\n",
        "wget -q 'https://www.dropbox.com/s/52fiyn199kgk0iq/datasets_combined.pkl?dl=1' -O 'datasets_combined.pkl'\n",
        "mkdir train\n",
        "cd train\n",
        "wget -q 'https://www.dropbox.com/s/cnw462g0oyo28i0/lcp_single_train.tsv?dl=1' -O 'lcp_single_train.tsv'\n",
        "wget -q 'https://www.dropbox.com/s/y1yoq24hzqbe5bf/lcp_multi_train.tsv?dl=1' -O 'lcp_multi_train.tsv'\n",
        "cd ..\n",
        "mkdir trial\n",
        "cd trial\n",
        "wget -q 'https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/trial/lcp_single_trial.tsv' -O 'lcp_single_trial.tsv'\n",
        "wget -q 'https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/trial/lcp_multi_trial.tsv' -O 'lcp_multi_trial.tsv'\n",
        "cd ..\n",
        "mkdir test\n",
        "cd test\n",
        "wget -q 'https://www.dropbox.com/s/mjcwx9wawealjk8/lcp_single_test.tsv?dl=1' -O 'lcp_single_test.tsv'\n",
        "wget -q 'https://www.dropbox.com/s/zil1h9xp7hrhkw1/lcp_multi_test.tsv?dl=1' -O 'lcp_multi_test.tsv'\n",
        "\n",
        "wget -q 'https://www.dropbox.com/s/n2ix1cxkmjqxmdu/lcp_single_test.tsv?dl=0' -O 'lcp_single_test_labels.tsv'\n",
        "wget -q 'https://www.dropbox.com/s/zwrv1dqvip1x6xr/lcp_multi_test.tsv?dl=0' -O 'lcp_multi_test_labels.tsv'\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'sample_data': No such file or directory\n",
            "mkdir: cannot create directory ‘train’: File exists\n",
            "mkdir: cannot create directory ‘trial’: File exists\n",
            "mkdir: cannot create directory ‘test’: File exists\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wBbkDm5iVE6"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "import copy\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import BertModel, RobertaModel, BertTokenizer, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, random_split, DataLoader, IterableDataset, ConcatDataset\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import f1_score \n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PZg53Ino1L5"
      },
      "source": [
        "data = pd.read_csv('train/lcp_multi_train.tsv',  delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "\n",
        "#for Subtask 2"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0nc-Bqxfdah"
      },
      "source": [
        "# data = pd.read_csv('train/lcp_single_train.tsv',  delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "\n",
        "## For subtask 1"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEncOaduJH2W"
      },
      "source": [
        "for i, token in enumerate(list(data.token)):\n",
        "  if isinstance(token, float):\n",
        "    data.drop([i], inplace = True)\n",
        "    # print('Yes')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUX9gD4hdfza"
      },
      "source": [
        "tokens = [ ]\n",
        "# print(data.token)\n",
        "for en in data.token:\n",
        "  \n",
        "  x = en.replace(\" \", \"\")\n",
        "  tokens.append(x)\n",
        "data.token =  tokens\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZJ0FIU7dmmm",
        "outputId": "be99e27c-9e7a-48ab-ab48-271699c607a7"
      },
      "source": [
        "max_length = -1\n",
        "for token in data.token:\n",
        "  if len(token) > max_length:\n",
        "    max_length = len(token)\n",
        "\n",
        "print(max_length)    "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODruPqoOebyl"
      },
      "source": [
        "# DataSet Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N-roJHCLlQ9"
      },
      "source": [
        "class Dataset():\n",
        "    def __init__(self, df, batch_size = 32, max_len = 128):\n",
        "        self.max_len = max_len\n",
        "        self.sentences = list(df.sentence)\n",
        "        self.tokens = list(df.token)\n",
        "        self.encoded_tokens, self.X_lengths = self.one_hot_batch(self.encode_tokens(list(df.token))[0])\n",
        "        \n",
        "        self.encoded_tokens = torch.FloatTensor(self.encoded_tokens)\n",
        "        self.X_lengths = torch.FloatTensor(self.X_lengths)\n",
        "        \n",
        "        self.complexity = torch.Tensor(list(df.complexity))\n",
        "        self.batch_size = batch_size\n",
        "        self.train_dataloader, self.val_dataloader = self.process_data()\n",
        "\n",
        "    def one_hot_word(self, encoded_tokens):\n",
        "        identity = torch.eye(26)\n",
        "        res = torch.Tensor(identity[encoded_tokens[0] - 1]).view(1, 26)\n",
        "        for each in (encoded_tokens)[1:]:\n",
        "          if each == 0:\n",
        "            res = torch.cat([res, torch.zeros(1, 26)])\n",
        "          else:\n",
        "            res = torch.cat([res, identity[each - 1].view(1, 26)])\n",
        "        return res\n",
        "\n",
        "    def one_hot_batch(self, encoded_tokens):\n",
        "        X_lengths = self.get_lengths(encoded_tokens)\n",
        "        final_res = self.one_hot_word(encoded_tokens[0]).view(1, max_length, 26)\n",
        "        for i in range(encoded_tokens.shape[0] - 1):\n",
        "          one_hot_res = self.one_hot_word(encoded_tokens[i + 1])\n",
        "          final_res = torch.cat([final_res, one_hot_res.view(1, max_length, 26)], dim = 0)\n",
        "        return final_res, X_lengths\n",
        "\n",
        "    def get_lengths(self, x):\n",
        "        X_lengths = []\n",
        "        for i in range(len(x)):\n",
        "            num = 0\n",
        "            while (num != max_length and x[i][num] != 0):\n",
        "                num += 1\n",
        "            X_lengths.append(num)\n",
        "        return X_lengths\n",
        "\n",
        "    def encode_tokens(self, tokens):\n",
        "        n_labels = 26\n",
        "        text = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        chars = tuple(set(text))\n",
        "        intrange = set(i + 1 for i in range(26))\n",
        "        int2char = dict(zip(intrange, chars))\n",
        "        char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "        encoded_list = []\n",
        "        for token in tokens:\n",
        "          token  = token.lower()\n",
        "          tokenx = token.split()\n",
        "          token  = \"\"\n",
        "          for i in range(len(tokenx)):\n",
        "            token=token+tokenx[i]\n",
        "          encoded = np.array([char2int[ch] for ch in token])\n",
        "          while len(encoded)<max_length:\n",
        "            encoded= np.append(encoded,0)\n",
        "          encoded_list.append(torch.from_numpy(encoded))\n",
        "          padded_list, mask = self.padding_tensor(encoded_list)\n",
        "        print(padded_list[0])\n",
        "        return padded_list, mask\n",
        "\n",
        "    def padding_tensor(self, sequences):\n",
        "        num = len(sequences)\n",
        "        max_len = max([s.size(0) for s in sequences])\n",
        "        out_dims = (num, max_len)\n",
        "        out_tensor = sequences[0].data.new(*out_dims).fill_(0)\n",
        "        mask = sequences[0].data.new(*out_dims).fill_(0)\n",
        "        for i, tensor in enumerate(sequences):\n",
        "            length = tensor.size(0)\n",
        "            out_tensor[i, :length] = tensor\n",
        "            mask[i, :length] = 1\n",
        "        return out_tensor, mask\n",
        "    \n",
        "      \n",
        "    def process_data(self):\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        input_ids, attention_masks = [], []\n",
        "        for sentence, token in zip(self.sentences, self.tokens):\n",
        "            sent = str(token).lower() + ' [SEP] ' +  str(sentence).lower() \n",
        "            encoded_dict = tokenizer.encode_plus(sent,\n",
        "                                                    add_special_tokens=True,\n",
        "                                                    max_length=self.max_len, \n",
        "                                                    padding='max_length', \n",
        "                                                    return_attention_mask = True,\n",
        "                                                    return_tensors = 'pt', \n",
        "                                                    truncation = True)\n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        \n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "        return self.get_dataloaders(input_ids, attention_masks)\n",
        "    \n",
        "    def get_dataloaders(self, input_ids, attention_masks):\n",
        "        input_ids, attention_masks, self.complexity, self.encoded_tokens, self.X_lengths = sklearn.utils.shuffle(input_ids, attention_masks, self.complexity, self.encoded_tokens, self.X_lengths, random_state=42)\n",
        "        train_idx, test_idx, _, _ = train_test_split(range(input_ids.shape[0]), input_ids, test_size = 0.2, random_state = 42)\n",
        "\n",
        "        training_data = TensorDataset(input_ids[train_idx], attention_masks[train_idx], self.complexity[train_idx], self.encoded_tokens[train_idx], self.X_lengths[train_idx])\n",
        "        training_sampler = RandomSampler(training_data)\n",
        "        training_dataloader = DataLoader(training_data, sampler=training_sampler, batch_size=self.batch_size)\n",
        "\n",
        "        test_data = TensorDataset(input_ids[test_idx], attention_masks[test_idx], self.complexity[test_idx], self.encoded_tokens[test_idx], self.X_lengths[test_idx])\n",
        "        test_sampler = SequentialSampler(test_data)\n",
        "        test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=self.batch_size)\n",
        "\n",
        "\n",
        "        return training_dataloader, test_dataloader"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puX_FSsZq9kt",
        "outputId": "ecb23ab1-41bc-440d-9a39-e47f730f3017"
      },
      "source": [
        "start_time = time.time()\n",
        "dataset = Dataset(data, batch_size = 32)\n",
        "print(\"Time taken: \" + str((time.time() - start_time)/60))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([18, 17,  3, 17, 16, 23, 11, 12,  8, 14,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
            "Time taken: 0.1912899374961853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW_kzXLXeXfn"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFc98ehrPliJ"
      },
      "source": [
        "class BERT_CharLSTM(torch.nn.Module):\n",
        "    def __init__(self, BERT_in, n_hidden=100, n_layers=2, seq_length=max_length, n_linear=500, drop_prob=0.25, lr=0.001):\n",
        "        \n",
        "        super(BERT_CharLSTM, self).__init__()\n",
        "\n",
        "        # CharLSTM\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_linear = n_linear\n",
        "        self.seq_length = seq_length\n",
        "        self.lr = lr\n",
        "\n",
        "        self.chars = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True, bidirectional = True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "      \n",
        "        self.fc = nn.Linear(2*n_hidden*seq_length, n_linear, bias = True)\n",
        "\n",
        "        # BERT\n",
        "        self.embeddings = BertModel.from_pretrained('bert-base-uncased',  output_hidden_states = True)\n",
        "        self.final = nn.Linear(BERT_in, n_linear, bias = True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Combined\n",
        "        self.final_1 = nn.Linear(2 * n_linear, 1, bias = True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "        hidden = ((torch.zeros(2 * self.n_layers, batch_size, self.n_hidden)).to(device),\n",
        "                  (torch.zeros(2 * self.n_layers, batch_size, self.n_hidden)).to(device))\n",
        "\n",
        "        return hidden\n",
        "\n",
        "    def forward(self, x, x_mask, X_char, X_char_lengths, batch_size, hidden):\n",
        "\n",
        "        # CharLSTM\n",
        "        X = X_char\n",
        "        r_output, hidden = self.lstm(X, hidden)\n",
        "        out = r_output\n",
        "        out = self.dropout(out)\n",
        "        out = out.contiguous()\n",
        "        out = self.fc(out.view(out.shape[0], -1)) \n",
        "        out = self.relu(out)\n",
        "        out = out.view(out.shape[0], -1) \n",
        "\n",
        "        # BERT\n",
        "        embed = self.embeddings(x,x_mask)[1]\n",
        "        y_pred = self.relu(self.final(self.dropout(embed)))\n",
        "        y_pred = y_pred.view(y_pred.shape[0], -1) \n",
        "        \n",
        "        # Combined\n",
        "        final_inp = torch.cat([out, y_pred], dim = 1) \n",
        "        final_inp = self.final_1(final_inp)\n",
        "        final_inp = final_inp.view(final_inp.shape[0])\n",
        "        final_result = self.sigmoid(final_inp)\n",
        "\n",
        "        return final_result"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCjQ3-AcQPyJ"
      },
      "source": [
        "def save_metrics(save_path, epochs, model, optimizer, L1):\n",
        "\n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'epochs': epochs+1,\n",
        "                  'L1': L1}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path, model, optimizer):\n",
        "    try: \n",
        "        state_dict = torch.load(load_path, map_location=device)\n",
        "        model.load_state_dict(state_dict['model_state_dict'])\n",
        "        optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    except: \n",
        "        state_dict = {}\n",
        "\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict.get('epochs', 0), state_dict.get('L1', 1000)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMvlV4mly7yT"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn7tWK0FQQot"
      },
      "source": [
        "def evaluate(test_dataloader, model):\n",
        "    model.eval()\n",
        "    total_eval_accuracy=0\n",
        "    y_preds = np.array([])\n",
        "    y_test = np.array([])\n",
        "    total_loss = 0\n",
        "    criterion = nn.L1Loss()\n",
        "    hidden_charlstm = model.init_hidden(batch_size = 32)\n",
        "    for batch in test_dataloader:\n",
        "        hidden_charlstm = model.init_hidden(batch_size = batch[0].to(device).shape[0])\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_X_char = batch[3].to(device)\n",
        "        b_X_char_lengths = batch[4].to(device)\n",
        "        with torch.no_grad():\n",
        "            ypred = model(b_input_ids, b_input_mask, b_X_char, b_X_char_lengths.cpu(), batch_size = 32, hidden = hidden_charlstm)        \n",
        "\n",
        "        ypred = ypred.to('cpu').numpy()\n",
        "        b_labels = b_labels.to('cpu').numpy()\n",
        "\n",
        "        y_preds = np.hstack((y_preds, ypred))\n",
        "        y_test = np.hstack((y_test, b_labels))\n",
        "\n",
        "    loss = np.mean(np.abs(y_preds-y_test))\n",
        "    corr, _ = pearsonr(y_preds, y_test)\n",
        "    return loss, y_preds, y_test, corr\n",
        " \n",
        "def train(training_dataloader, validation_dataloader, model, filename, epochs = 4):\n",
        "    total_steps = len(training_dataloader) * epochs\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps = 1e-8)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                num_training_steps = total_steps)\n",
        "    \n",
        "    criterion = nn.L1Loss()\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    hidden_charlstm = model.init_hidden(batch_size = 32)\n",
        "\n",
        "    cur_epoch, best_l1 = load_metrics(filename, model, optimizer)\n",
        "    for epoch_i in tqdm(range(0, epochs)):\n",
        "        total_train_loss = 0\n",
        "        model.train()\n",
        "        for step, batch in enumerate(training_dataloader):\n",
        "            hidden_charlstm = model.init_hidden(batch_size = batch[0].to(device).shape[0])\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "            b_X_char = batch[3].to(device)\n",
        "            b_X_char_lengths = batch[4].to(device)\n",
        "            outputs = model(b_input_ids, b_input_mask, b_X_char, b_X_char_lengths.cpu(), batch_size = 32, hidden = hidden_charlstm)\n",
        "            loss = criterion(outputs, b_labels)\n",
        " \n",
        "            if step%50 == 0:\n",
        "                print(loss)\n",
        " \n",
        "            total_train_loss += loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        " \n",
        "        print()\n",
        "        print(f'Total Train Loss = {total_train_loss}')\n",
        "        print('#############    Validation Set Stats')\n",
        "        l1_loss, _, _, corr = evaluate(validation_dataloader, model)\n",
        "        print(\"  L1 loss: {}\".format(l1_loss))\n",
        "        print(\"  Pearson correlation: {}\".format(corr))\n",
        " \n",
        "        if l1_loss < best_l1:\n",
        "            best_l1 = l1_loss\n",
        "            save_metrics(filename, epoch_i, model, optimizer, l1_loss)\n",
        " "
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3zctP8gQSz_"
      },
      "source": [
        "model = BERT_CharLSTM(768).to(device)\n",
        "# print(model)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3-OFBnMyshq"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Visualize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sKgZ8jUrjRM"
      },
      "source": [
        "# list(model.parameters())[0].shape"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgkvp7weviYb"
      },
      "source": [
        "# batch = next(iter(dataset.train_dataloader))\n",
        "# b_input_ids = batch[0].to(device)\n",
        "# b_input_mask = batch[1].to(device)\n",
        "# b_labels = batch[2].to(device)\n",
        "# b_X_char = batch[3].to(device)\n",
        "# b_X_char_lengths = batch[4].to(device)\n",
        "# hidden_charlstm = model.init_hidden(batch_size = batch[0].to(device).shape[0])\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQTGp6bGwgGV"
      },
      "source": [
        "# yhat =  model(b_input_ids, b_input_mask, b_X_char, b_X_char_lengths.cpu(), batch_size = 32, hidden = hidden_charlstm)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_HXtKHvum5t"
      },
      "source": [
        "# from torchviz import make_dot\n",
        "\n",
        "# make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjRobExlzwGg"
      },
      "source": [
        "# import torch\n",
        "# import torchvision\n",
        "\n",
        "\n",
        "# shape_of_first_layer = list(model.parameters())[0].shape #shape_of_first_layer\n",
        "\n",
        "# N,C = shape_of_first_layer[:2]\n",
        "\n",
        "# dummy_input = torch.Tensor(N,C)\n",
        "\n",
        "# dummy_input = dummy_input[...,:, None,None] #adding the None for height and weight\n",
        "\n",
        "# summary(yhat, (400,26))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nwPEGRW0-XT"
      },
      "source": [
        "# print(model)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05KkPtoTzAun"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkk6EAfHIbX_",
        "outputId": "3dfa9db7-d884-4c3b-844d-37e47b491dad"
      },
      "source": [
        "train(dataset.train_dataloader, dataset.val_dataloader, model, 'lcp_sigmoid.pt')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model loaded from <== lcp_sigmoid.pt\n",
            "tensor(0.0913, device='cuda:0', grad_fn=<L1LossBackward>)\n",
            "\n",
            "Total Train Loss = 2.7638988494873047\n",
            "#############    Validation Set Stats\n",
            "  L1 loss: 0.08798573848693386\n",
            "  Pearson correlation: 0.6648753883632655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 25%|██▌       | 1/4 [00:30<01:31, 30.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved to ==> lcp_sigmoid.pt\n",
            "tensor(0.0652, device='cuda:0', grad_fn=<L1LossBackward>)\n",
            "\n",
            "Total Train Loss = 2.643728256225586\n",
            "#############    Validation Set Stats\n",
            "  L1 loss: 0.08222458978477669\n",
            "  Pearson correlation: 0.695147036928662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 50%|█████     | 2/4 [01:01<01:01, 30.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved to ==> lcp_sigmoid.pt\n",
            "tensor(0.0525, device='cuda:0', grad_fn=<L1LossBackward>)\n",
            "\n",
            "Total Train Loss = 2.3185789585113525\n",
            "#############    Validation Set Stats\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 75%|███████▌  | 3/4 [01:27<00:29, 29.29s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  L1 loss: 0.0840918795867382\n",
            "  Pearson correlation: 0.6930671257777912\n",
            "tensor(0.0485, device='cuda:0', grad_fn=<L1LossBackward>)\n",
            "\n",
            "Total Train Loss = 2.1249423027038574\n",
            "#############    Validation Set Stats\n",
            "  L1 loss: 0.08057602187373529\n",
            "  Pearson correlation: 0.7131870395437989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 4/4 [01:59<00:00, 29.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved to ==> lcp_sigmoid.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYWFZDrnfNwr"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZqMx-0VF34V"
      },
      "source": [
        "test_data = pd.read_csv('/content/test/lcp_multi_test.tsv',  delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "\n",
        "##For Subtask 2\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ScUs8n-fQyp"
      },
      "source": [
        "# test_data = pd.read_csv('/content/test/lcp_single_test.tsv',  delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
        "\n",
        "##For Subtask 1\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxlonJoJh9k4"
      },
      "source": [
        "tokens = [ ]\n",
        "# print(data.token)\n",
        "for en in test_data.token:  \n",
        "  x = en.replace(\" \", \"\")\n",
        "  tokens.append(x)\n",
        "test_data.token =  tokens\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZiPe7bYjQfg"
      },
      "source": [
        "for i, word in enumerate(list(test_data.token)):\n",
        "  ''.join(e for e in word if e.isalnum())\n",
        "  if len(word) > max_length:\n",
        "    test_data.token[i] = word[:max_length]"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27EK0LoRaZBT"
      },
      "source": [
        "test_data.to_csv('test_original.csv')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLrhDZdSjoU8",
        "outputId": "7f2c1532-bcdb-48c7-8136-98ead5c1d7d6"
      },
      "source": [
        "print(max_length)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goQZBBcTuDbI",
        "outputId": "e6a8f167-9232-46c6-c85e-186bb8f319bc"
      },
      "source": [
        "test_max_length = -1\n",
        "for token in test_data.token:\n",
        "  if len(token) > test_max_length:\n",
        "    test_max_length = len(token)\n",
        "\n",
        "print(test_max_length)    "
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWwmgM0nRxFu"
      },
      "source": [
        "class TestDataset():\n",
        "    def __init__(self, df, batch_size = 32, max_len = 128):\n",
        "        self.max_len = max_len\n",
        "        self.sentences = list(df.sentence)\n",
        "        self.tokens = list(df.token)\n",
        "        self.encoded_tokens, self.X_lengths = self.one_hot_batch(self.encode_tokens(list(df.token))[0])\n",
        "        \n",
        "        self.encoded_tokens = torch.FloatTensor(self.encoded_tokens)\n",
        "        self.X_lengths = torch.FloatTensor(self.X_lengths)\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.test_dataloader = self.process_data()\n",
        "\n",
        "    def one_hot_word(self, encoded_tokens):\n",
        "        identity = torch.eye(26)\n",
        "        res = torch.Tensor(identity[encoded_tokens[0] - 1]).view(1, 26)\n",
        "        for each in (encoded_tokens)[1:]:\n",
        "          if each == 0:\n",
        "            res = torch.cat([res, torch.zeros(1, 26)])\n",
        "          else:\n",
        "            res = torch.cat([res, identity[each - 1].view(1, 26)])\n",
        "        return res\n",
        "\n",
        "    def one_hot_batch(self, encoded_tokens):\n",
        "        X_lengths = self.get_lengths(encoded_tokens)\n",
        "        final_res = self.one_hot_word(encoded_tokens[0]).view(1, max_length, 26)\n",
        "        for i in range(encoded_tokens.shape[0] - 1):\n",
        "          one_hot_res = self.one_hot_word(encoded_tokens[i + 1])\n",
        "          final_res = torch.cat([final_res, one_hot_res.view(1, max_length, 26)], dim = 0)\n",
        "        return final_res, X_lengths\n",
        "\n",
        "    def get_lengths(self, x):\n",
        "        X_lengths = []\n",
        "        for i in range(len(x)):\n",
        "            num = 0\n",
        "            while (num != test_max_length and x[i][num] != 0):\n",
        "                num += 1\n",
        "            X_lengths.append(num)\n",
        "        return X_lengths\n",
        "\n",
        "    def encode_tokens(self, tokens):\n",
        "        n_labels = 26\n",
        "        text = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        chars = tuple(set(text))\n",
        "        intrange = set(i + 1 for i in range(26))\n",
        "        int2char = dict(zip(intrange, chars))\n",
        "        char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "        encoded_list = []\n",
        "        for token in tokens:\n",
        "          token  = token.lower()\n",
        "          tokenx = token.split()\n",
        "          token  = \"\"\n",
        "          for i in range(len(tokenx)):\n",
        "            token=token+tokenx[i]\n",
        "          encoded = np.array([char2int[ch] for ch in token])\n",
        "          while len(encoded)<max_length:\n",
        "            encoded= np.append(encoded,0)\n",
        "          encoded_list.append(torch.from_numpy(encoded))\n",
        "          padded_list, mask = self.padding_tensor(encoded_list)\n",
        "        print(padded_list[0])\n",
        "        return padded_list, mask\n",
        "\n",
        "    def padding_tensor(self, sequences):\n",
        "        num = len(sequences)\n",
        "        max_len = max([s.size(0) for s in sequences])\n",
        "        out_dims = (num, max_len)\n",
        "        out_tensor = sequences[0].data.new(*out_dims).fill_(0)\n",
        "        mask = sequences[0].data.new(*out_dims).fill_(0)\n",
        "        for i, tensor in enumerate(sequences):\n",
        "            length = tensor.size(0)\n",
        "            out_tensor[i, :length] = tensor\n",
        "            mask[i, :length] = 1\n",
        "        return out_tensor, mask\n",
        "    \n",
        "      \n",
        "    def process_data(self):\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        input_ids, attention_masks = [], []\n",
        "        for sentence, token in zip(self.sentences, self.tokens):\n",
        "            sent = str(token).lower() + ' [SEP] ' +  str(sentence).lower() \n",
        "            encoded_dict = tokenizer.encode_plus(sent,\n",
        "                                                    add_special_tokens=True,\n",
        "                                                    max_length=self.max_len, \n",
        "                                                    padding='max_length', \n",
        "                                                    return_attention_mask = True,\n",
        "                                                    return_tensors = 'pt', \n",
        "                                                    truncation = True)\n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "        \n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "        return self.get_dataloaders(input_ids, attention_masks)\n",
        "    \n",
        "    def get_dataloaders(self, input_ids, attention_masks):\n",
        "        test_data = TensorDataset(input_ids, attention_masks, self.encoded_tokens, self.X_lengths)\n",
        "        test_sampler = SequentialSampler(test_data)\n",
        "        test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=self.batch_size)\n",
        "        return test_dataloader"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxX9JydxUtR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320e06d9-f613-4e01-89d3-69d7d76e9863"
      },
      "source": [
        "test_dataset = TestDataset(test_data, batch_size = 32)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1, 11,  6, 17, 19,  5, 25,  6, 17, 18, 23, 18,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89AkAC0Qm5fl"
      },
      "source": [
        ""
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V00_xJkNScGy"
      },
      "source": [
        "def evaluate(test_dataloader, model):\n",
        "    model.eval()\n",
        "    y_preds = np.array([])\n",
        "    total_loss = 0\n",
        "    hidden_charlstm = model.init_hidden(batch_size = 32)\n",
        "    for batch in test_dataloader:\n",
        "        hidden_charlstm = model.init_hidden(batch_size = batch[0].to(device).shape[0])\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_X_char = batch[2].to(device)\n",
        "        b_X_char_lengths = batch[3].to(device)\n",
        "        with torch.no_grad():\n",
        "            ypred = model(b_input_ids, b_input_mask, b_X_char, b_X_char_lengths.cpu(), batch_size = 32, hidden = hidden_charlstm)        \n",
        "        ypred = ypred.to('cpu').numpy()\n",
        "        y_preds = np.append(y_preds,ypred)\n",
        "\n",
        "    return y_preds"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skwTTsB4cvbv"
      },
      "source": [
        "model = BERT_CharLSTM(768).to(device)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4DBdVjBdN_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ab6294-e409-4e83-88bb-bdc2a80a1bd8"
      },
      "source": [
        "state_dict = torch.load(f = '/content/lcp_sigmoid.pt', map_location=device)\n",
        "model.load_state_dict(state_dict['model_state_dict'])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYD-1vvNsi6O"
      },
      "source": [
        ""
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjT19zUvcrcc"
      },
      "source": [
        "FINAL_PREDS = evaluate(test_dataset.test_dataloader, model)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzWwUPKteX1V"
      },
      "source": [
        "results_dataset = pd.DataFrame()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1lbmg1mkCHh"
      },
      "source": [
        "results_dataset['id'] = test_data['id']"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0AauEAIkfdr",
        "outputId": "d8fd1c40-dcfd-41ce-f6dd-e66f3ebc66a8"
      },
      "source": [
        "len(test_data.id)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuvxAcyokFsL",
        "outputId": "3f0638af-3ac3-4bc8-ede6-4470e9d98aac"
      },
      "source": [
        "len(FINAL_PREDS)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGGCiRE2e_7A"
      },
      "source": [
        "results_dataset['complexity'] = list(FINAL_PREDS)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqR-LYSgfGUd"
      },
      "source": [
        "results_dataset.to_csv('results_latest.csv',index=False,header=False)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hir1CgeOsH42"
      },
      "source": [
        ""
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAdsHb9PrYB1"
      },
      "source": [
        "single_test_labels = pd.read_csv('test/lcp_single_test_labels.tsv',  delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QIVUq4GrlT2"
      },
      "source": [
        "multi_test_labels = pd.read_csv('test/lcp_multi_test_labels.tsv',  delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuhaKZXhsP7U"
      },
      "source": [
        ""
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdmjkUCAsP-R"
      },
      "source": [
        "results_dataset = results_dataset.sort_values(\"id\")\n",
        "multi_test_labels = multi_test_labels.sort_values(\"id\")\n",
        "single_test_labels = single_test_labels.sort_values('id')"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cTg00B6eLiG"
      },
      "source": [
        "# Results of SubTask 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGc62qR2fwtc"
      },
      "source": [
        "# #for single token Subtask 1\n",
        "# import scipy\n",
        "# import scipy.stats\n",
        "# y1 = results_dataset['complexity']\n",
        "# y2 = single_test_labels['complexity']\n",
        "# scipy.stats.spearmanr(y1,y2)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc5A9eRKfnsA"
      },
      "source": [
        "# Results of Subtask 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS8MSgETsOQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa8efb3-9e1e-4a66-b621-aea813bcd854"
      },
      "source": [
        "# #for multi\n",
        "# import scipy\n",
        "# import scipy.stats\n",
        "# y1 = results_dataset['complexity']\n",
        "# y2 = multi_test_labels['complexity']\n",
        "# scipy.stats.spearmanr(y1,y2)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SpearmanrResult(correlation=0.7074018320973854, pvalue=3.1087405237876893e-29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL2g_A94sqxX"
      },
      "source": [
        ""
      ],
      "execution_count": 94,
      "outputs": []
    }
  ]
}